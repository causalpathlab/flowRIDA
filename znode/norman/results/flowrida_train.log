2024-12-27 12:03:10 INFO     Starting training...
2024-12-27 12:03:10 INFO     {'device': 'cuda', 'input_dim': 5000, 'latent_dim': 15, 'layers': [200, 100, 50, 15], 'learning_rate': 0.001, 'epochs': 1000, 'batch_size': 128}
2024-12-27 12:03:10 INFO     ETM(
  (encoder): ETMEncoder(
    (fc): Stacklayers(
      (layers): ModuleList(
        (0): Linear(in_features=5000, out_features=200, bias=True)
        (1): ReLU()
        (2): Linear(in_features=200, out_features=100, bias=True)
        (3): ReLU()
        (4): Linear(in_features=100, out_features=50, bias=True)
        (5): ReLU()
        (6): Linear(in_features=50, out_features=15, bias=True)
        (7): ReLU()
      )
    )
    (z_mean): Linear(in_features=15, out_features=15, bias=True)
    (z_lnvar): Linear(in_features=15, out_features=15, bias=True)
  )
  (decoder): ETMDecoder(
    (lsmax): LogSoftmax(dim=-1)
  )
)
2024-12-27 12:03:10 INFO     Starting training....
2024-12-27 12:03:12 INFO     ====> Epoch: 0 Average loss: 3260.4932
2024-12-27 12:03:21 INFO     ====> Epoch: 10 Average loss: 2729.3111
2024-12-27 12:03:39 INFO     Starting training...
2024-12-27 12:03:39 INFO     {'device': 'cuda', 'input_dim': 5000, 'latent_dim': 15, 'layers': [200, 100, 50, 15], 'learning_rate': 0.001, 'epochs': 10, 'batch_size': 128}
2024-12-27 12:03:39 INFO     ETM(
  (encoder): ETMEncoder(
    (fc): Stacklayers(
      (layers): ModuleList(
        (0): Linear(in_features=5000, out_features=200, bias=True)
        (1): ReLU()
        (2): Linear(in_features=200, out_features=100, bias=True)
        (3): ReLU()
        (4): Linear(in_features=100, out_features=50, bias=True)
        (5): ReLU()
        (6): Linear(in_features=50, out_features=15, bias=True)
        (7): ReLU()
      )
    )
    (z_mean): Linear(in_features=15, out_features=15, bias=True)
    (z_lnvar): Linear(in_features=15, out_features=15, bias=True)
  )
  (decoder): ETMDecoder(
    (lsmax): LogSoftmax(dim=-1)
  )
)
2024-12-27 12:03:39 INFO     Starting training....
2024-12-27 12:03:41 INFO     ====> Epoch: 0 Average loss: 3260.4932
2024-12-27 12:03:49 INFO     Completed training...model saved in results/nn_flowrida.model
2024-12-27 12:03:49 INFO     Starting eval...
2024-12-27 12:03:49 INFO     {'device': 'cuda', 'input_dim': 5000, 'latent_dim': 15, 'layers': [200, 100, 50, 15], 'learning_rate': 0.001, 'epochs': 10, 'batch_size': 128}
2024-12-27 12:03:49 INFO     ETM(
  (encoder): ETMEncoder(
    (fc): Stacklayers(
      (layers): ModuleList(
        (0): Linear(in_features=5000, out_features=200, bias=True)
        (1): ReLU()
        (2): Linear(in_features=200, out_features=100, bias=True)
        (3): ReLU()
        (4): Linear(in_features=100, out_features=50, bias=True)
        (5): ReLU()
        (6): Linear(in_features=50, out_features=15, bias=True)
        (7): ReLU()
      )
    )
    (z_mean): Linear(in_features=15, out_features=15, bias=True)
    (z_lnvar): Linear(in_features=15, out_features=15, bias=True)
  )
  (decoder): ETMDecoder(
    (lsmax): LogSoftmax(dim=-1)
  )
)
2024-12-27 12:05:18 INFO     Starting training...
2024-12-27 12:05:18 INFO     {'device': 'cuda', 'input_dim': 5000, 'latent_dim': 15, 'layers': [200, 100, 50, 15], 'learning_rate': 0.001, 'epochs': 1000, 'batch_size': 128}
2024-12-27 12:05:18 INFO     ETM(
  (encoder): ETMEncoder(
    (fc): Stacklayers(
      (layers): ModuleList(
        (0): Linear(in_features=5000, out_features=200, bias=True)
        (1): ReLU()
        (2): Linear(in_features=200, out_features=100, bias=True)
        (3): ReLU()
        (4): Linear(in_features=100, out_features=50, bias=True)
        (5): ReLU()
        (6): Linear(in_features=50, out_features=15, bias=True)
        (7): ReLU()
      )
    )
    (z_mean): Linear(in_features=15, out_features=15, bias=True)
    (z_lnvar): Linear(in_features=15, out_features=15, bias=True)
  )
  (decoder): ETMDecoder(
    (lsmax): LogSoftmax(dim=-1)
  )
)
2024-12-27 12:05:18 INFO     Starting training....
2024-12-27 12:05:20 INFO     ====> Epoch: 0 Average loss: 3260.4932
2024-12-27 12:05:29 INFO     ====> Epoch: 10 Average loss: 2729.3111
2024-12-27 12:05:38 INFO     ====> Epoch: 20 Average loss: 2552.0354
2024-12-27 12:05:47 INFO     ====> Epoch: 30 Average loss: 2504.4555
2024-12-27 12:05:57 INFO     ====> Epoch: 40 Average loss: 2479.2830
2024-12-27 12:06:06 INFO     ====> Epoch: 50 Average loss: 2468.8298
2024-12-27 12:06:15 INFO     ====> Epoch: 60 Average loss: 2463.5110
2024-12-27 12:06:24 INFO     ====> Epoch: 70 Average loss: 2454.7229
2024-12-27 12:06:33 INFO     ====> Epoch: 80 Average loss: 2454.5603
2024-12-27 12:06:42 INFO     ====> Epoch: 90 Average loss: 2447.3787
2024-12-27 12:06:51 INFO     ====> Epoch: 100 Average loss: 2445.9076
2024-12-27 12:07:01 INFO     ====> Epoch: 110 Average loss: 2443.2889
2024-12-27 12:07:10 INFO     ====> Epoch: 120 Average loss: 2441.6521
2024-12-27 12:07:19 INFO     ====> Epoch: 130 Average loss: 2438.4497
2024-12-27 12:07:28 INFO     ====> Epoch: 140 Average loss: 2437.7918
2024-12-27 12:07:37 INFO     ====> Epoch: 150 Average loss: 2436.7961
2024-12-27 12:07:46 INFO     ====> Epoch: 160 Average loss: 2434.7854
2024-12-27 12:07:55 INFO     ====> Epoch: 170 Average loss: 2433.8076
2024-12-27 12:08:04 INFO     ====> Epoch: 180 Average loss: 2431.3260
2024-12-27 12:08:13 INFO     ====> Epoch: 190 Average loss: 2432.2513
2024-12-27 12:08:22 INFO     ====> Epoch: 200 Average loss: 2434.9870
2024-12-27 12:08:32 INFO     ====> Epoch: 210 Average loss: 2430.9497
2024-12-27 12:08:41 INFO     ====> Epoch: 220 Average loss: 2427.2967
2024-12-27 12:08:50 INFO     ====> Epoch: 230 Average loss: 2428.8045
2024-12-27 12:08:59 INFO     ====> Epoch: 240 Average loss: 2429.6287
2024-12-27 12:09:08 INFO     ====> Epoch: 250 Average loss: 2430.0533
2024-12-27 12:09:17 INFO     ====> Epoch: 260 Average loss: 2426.6596
2024-12-27 12:09:26 INFO     ====> Epoch: 270 Average loss: 2426.9300
2024-12-27 12:09:35 INFO     ====> Epoch: 280 Average loss: 2427.4096
2024-12-27 12:09:44 INFO     ====> Epoch: 290 Average loss: 2426.5839
2024-12-27 12:09:53 INFO     ====> Epoch: 300 Average loss: 2425.2917
2024-12-27 12:10:02 INFO     ====> Epoch: 310 Average loss: 2428.3921
2024-12-27 12:10:11 INFO     ====> Epoch: 320 Average loss: 2429.0550
2024-12-27 12:10:20 INFO     ====> Epoch: 330 Average loss: 2426.2475
2024-12-27 12:10:29 INFO     ====> Epoch: 340 Average loss: 2428.0099
2024-12-27 12:10:39 INFO     ====> Epoch: 350 Average loss: 2425.4127
2024-12-27 12:10:48 INFO     ====> Epoch: 360 Average loss: 2423.6611
2024-12-27 12:10:57 INFO     ====> Epoch: 370 Average loss: 2426.1307
2024-12-27 12:11:06 INFO     ====> Epoch: 380 Average loss: 2425.0106
2024-12-27 12:11:15 INFO     ====> Epoch: 390 Average loss: 2423.8343
2024-12-27 12:11:24 INFO     ====> Epoch: 400 Average loss: 2426.8920
2024-12-27 12:11:33 INFO     ====> Epoch: 410 Average loss: 2423.0069
2024-12-27 12:11:42 INFO     ====> Epoch: 420 Average loss: 2423.5911
2024-12-27 12:11:51 INFO     ====> Epoch: 430 Average loss: 2424.4113
2024-12-27 12:12:00 INFO     ====> Epoch: 440 Average loss: 2423.4825
2024-12-27 12:12:09 INFO     ====> Epoch: 450 Average loss: 2421.7054
2024-12-27 12:12:19 INFO     ====> Epoch: 460 Average loss: 2423.1459
2024-12-27 12:12:28 INFO     ====> Epoch: 470 Average loss: 2423.1383
2024-12-27 12:12:37 INFO     ====> Epoch: 480 Average loss: 2422.1702
2024-12-27 12:12:46 INFO     ====> Epoch: 490 Average loss: 2423.5961
2024-12-27 12:12:55 INFO     ====> Epoch: 500 Average loss: 2421.7341
2024-12-27 12:13:04 INFO     ====> Epoch: 510 Average loss: 2421.7317
2024-12-27 12:13:13 INFO     ====> Epoch: 520 Average loss: 2422.7800
2024-12-27 12:13:22 INFO     ====> Epoch: 530 Average loss: 2421.9796
2024-12-27 12:13:31 INFO     ====> Epoch: 540 Average loss: 2424.3258
2024-12-27 12:13:40 INFO     ====> Epoch: 550 Average loss: 2420.9321
2024-12-27 12:13:50 INFO     ====> Epoch: 560 Average loss: 2422.8157
2024-12-27 12:13:59 INFO     ====> Epoch: 570 Average loss: 2421.7582
2024-12-27 12:14:08 INFO     ====> Epoch: 580 Average loss: 2422.2331
2024-12-27 12:14:17 INFO     ====> Epoch: 590 Average loss: 2421.1477
2024-12-27 12:14:26 INFO     ====> Epoch: 600 Average loss: 2423.6119
2024-12-27 12:14:35 INFO     ====> Epoch: 610 Average loss: 2422.1956
2024-12-27 12:14:44 INFO     ====> Epoch: 620 Average loss: 2424.8202
2024-12-27 12:14:53 INFO     ====> Epoch: 630 Average loss: 2421.8410
2024-12-27 12:15:02 INFO     ====> Epoch: 640 Average loss: 2421.7849
2024-12-27 12:15:11 INFO     ====> Epoch: 650 Average loss: 2420.7448
2024-12-27 12:15:20 INFO     ====> Epoch: 660 Average loss: 2420.9961
2024-12-27 12:15:30 INFO     ====> Epoch: 670 Average loss: 2422.6456
2024-12-27 12:15:39 INFO     ====> Epoch: 680 Average loss: 2420.8267
2024-12-27 12:15:48 INFO     ====> Epoch: 690 Average loss: 2422.7106
2024-12-27 12:15:57 INFO     ====> Epoch: 700 Average loss: 2423.8045
2024-12-27 12:16:06 INFO     ====> Epoch: 710 Average loss: 2421.5878
2024-12-27 12:16:15 INFO     ====> Epoch: 720 Average loss: 2419.5793
2024-12-27 12:16:24 INFO     ====> Epoch: 730 Average loss: 2422.1886
2024-12-27 12:16:33 INFO     ====> Epoch: 740 Average loss: 2418.8407
2024-12-27 12:16:42 INFO     ====> Epoch: 750 Average loss: 2420.9439
2024-12-27 12:16:51 INFO     ====> Epoch: 760 Average loss: 2423.2497
2024-12-27 12:17:01 INFO     ====> Epoch: 770 Average loss: 2421.5837
2024-12-27 12:17:10 INFO     ====> Epoch: 780 Average loss: 2420.9925
2024-12-27 12:17:19 INFO     ====> Epoch: 790 Average loss: 2421.7606
2024-12-27 12:17:28 INFO     ====> Epoch: 800 Average loss: 2421.3509
2024-12-27 12:17:37 INFO     ====> Epoch: 810 Average loss: 2421.4121
2024-12-27 12:17:46 INFO     ====> Epoch: 820 Average loss: 2421.3773
2024-12-27 12:17:55 INFO     ====> Epoch: 830 Average loss: 2421.0077
2024-12-27 12:18:04 INFO     ====> Epoch: 840 Average loss: 2420.3151
2024-12-27 12:18:13 INFO     ====> Epoch: 850 Average loss: 2420.7077
2024-12-27 12:18:23 INFO     ====> Epoch: 860 Average loss: 2420.3916
2024-12-27 12:18:32 INFO     ====> Epoch: 870 Average loss: 2419.0058
2024-12-27 12:18:41 INFO     ====> Epoch: 880 Average loss: 2422.1383
2024-12-27 12:18:50 INFO     ====> Epoch: 890 Average loss: 2422.3480
2024-12-27 12:18:59 INFO     ====> Epoch: 900 Average loss: 2419.4661
2024-12-27 12:19:08 INFO     ====> Epoch: 910 Average loss: 2419.3637
2024-12-27 12:19:17 INFO     ====> Epoch: 920 Average loss: 2420.1953
2024-12-27 12:19:26 INFO     ====> Epoch: 930 Average loss: 2421.6212
2024-12-27 12:19:35 INFO     ====> Epoch: 940 Average loss: 2421.7520
2024-12-27 12:19:44 INFO     ====> Epoch: 950 Average loss: 2421.6365
2024-12-27 12:19:53 INFO     ====> Epoch: 960 Average loss: 2421.1442
2024-12-27 12:20:02 INFO     ====> Epoch: 970 Average loss: 2419.7038
2024-12-27 12:20:12 INFO     ====> Epoch: 980 Average loss: 2419.6124
2024-12-27 12:20:21 INFO     ====> Epoch: 990 Average loss: 2421.1956
2024-12-27 12:20:29 INFO     Completed training...model saved in results/nn_flowrida.model
2024-12-27 12:20:29 INFO     Starting eval...
2024-12-27 12:20:29 INFO     {'device': 'cuda', 'input_dim': 5000, 'latent_dim': 15, 'layers': [200, 100, 50, 15], 'learning_rate': 0.001, 'epochs': 1000, 'batch_size': 128}
2024-12-27 12:20:29 INFO     ETM(
  (encoder): ETMEncoder(
    (fc): Stacklayers(
      (layers): ModuleList(
        (0): Linear(in_features=5000, out_features=200, bias=True)
        (1): ReLU()
        (2): Linear(in_features=200, out_features=100, bias=True)
        (3): ReLU()
        (4): Linear(in_features=100, out_features=50, bias=True)
        (5): ReLU()
        (6): Linear(in_features=50, out_features=15, bias=True)
        (7): ReLU()
      )
    )
    (z_mean): Linear(in_features=15, out_features=15, bias=True)
    (z_lnvar): Linear(in_features=15, out_features=15, bias=True)
  )
  (decoder): ETMDecoder(
    (lsmax): LogSoftmax(dim=-1)
  )
)
